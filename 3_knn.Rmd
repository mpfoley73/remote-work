---
title: "Struggles with Remote Work"
subtitle: "Section 3: kNN"
author: "Michael Foley"
date: "8/1/2021"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: flatly
    highlight: haddock
    fig_width: 9
    fig_caption: false
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r warning=FALSE, message=FALSE, class.source='fold-hide'}
library(tidyverse)
library(ggtext)
library(janitor)
library(flextable)
library(caret)
library(recipes)

buffer <- readRDS("./buffer.rds")
codebook <- readRDS("./codebook.rds")
```

The *k*-nearest neighbors (kNN) algorithm is a type of supervised ML algorithm that classifies observations by the "majority vote" of the *k* nearest neighbors. The value of *k* is a hyperparameter to optimize.

kNN is a non-parametric model, meaning it makes no assumptions about the underlying data. This is often an advantage in cases where data does not follow standard distributions. kNN is also "lazy" algorithm in that it uses the training data set data to classify future responses rather than using it to create classification rules. The qualities make kNN relatively straight-forward to understand.

## Set Up Workflow

I'll partition my data into an 80:20 train:test split.

```{r}
set.seed(801)
train_idx <- createDataPartition(buffer$struggle, p = 0.8, list = FALSE)
dat_train <- buffer[train_idx, ]
dat_test <- buffer[-train_idx, ]
```

and train a model with 10-fold CV. 

```{r}
train_control <- trainControl(
  method = "cv", 
  number = 10,
  savePredictions = "final",
  classProbs = TRUE
)
```

## Prep Model

My model data set variables are the outcome variable `struggle` plus 15 predictors. I'll drop the character `country` column, plus the less-informative `role` and `industry` columns, leaving `struggle` plus 12 predictors.

```{r}
mdl_vars <- dat_train %>% select(struggle, everything(), -c(country, role, industry)) %>% colnames()
mdl_vars
```

I'll use the recipe method to train.

```{r}
rcpe <- recipe(struggle ~ ., data = dat_train[, mdl_vars]) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_center(all_predictors()) %>%
  step_scale(all_predictors())

prep(rcpe, training = dat_train)
```

## Fit KNN

The KNN model has a single hyperparameter to fit: *k*.

```{r message=FALSE}
set.seed(1970)
mdl_knn <- train(
  rcpe,
  data = dat_train[, mdl_vars],
  method = "knn",
  trControl = train_control,
  tuneGrid = expand.grid(k = c(25, 50, 100, 150, 200, 250, 300, 350)),
  metric = "Accuracy"
)

mdl_knn
```

Prediction accuracy was maximized at *K* = 250. Kappa was maximized 50.

```{r}
plot(mdl_knn)
```

## Resampling Performance

The confusion matrix compares the predicted to actual values from the 10-CV at the optimal *k*.

```{r}
(confusion <- confusionMatrix(mdl_knn))
```

The model basically predicts *Unplugging* all the time (`r scales::percent(sum(confusion$table[5, ]) / 100, accuracy = .1)`). Not a bad guess, really. 

## Holdout Performance

Here is the model performance on the test data set.

```{r}
preds_knn <- bind_cols(
  # dat_test,
  # predict(mdl_knn, newdata = dat_test, type = "prob"),
  Predicted = predict(mdl_knn, newdata = dat_test, type = "raw"),
  Actual = dat_test$struggle
)

(confusion_test <- confusionMatrix(preds_knn$Predicted, reference = preds_knn$Actual))
```

Predictably, it predicted *Unplugging* (`r scales::percent(sum(confusion_test$table[5, ]) / sum(confusion_test$table), accuracy = .1)`) of the time, and was able to score an overall accuracy of `r scales::percent(confusion_test$overall["Accuracy"])`.

## Model Evaluation

I'm not sure how variable importance is calculated for kNN. The matrix below places high importance on whether remote work stemmed from the COVID pandemic.

```{r fig.height=10, fig.width=8.2}
varImp(mdl_knn) %>%
  pluck("importance") %>%
  rownames_to_column(var = "FactorLevel") %>%
  pivot_longer(-FactorLevel) %>%
  ggplot(aes(x = name, y = fct_rev(FactorLevel))) +
  geom_tile(aes(fill = value), show.legend = FALSE) +
  geom_text(aes(label = round(value, 0)), size = 3) +
  theme_light() +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Variable Importance", y = NULL, x = NULL)
```

